{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial - Autograd & MLP (Multi-layer perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "- autograd 패키지는 텐서의 모든 연산에 대한 자동 미분을 제공\n",
    "- 실행-기반-정의(define-by-run)프레임워크로, 코드를 어떻게 작성하여 실행하는냐에 따라 역전파가 정의된다는 것을 의미\n",
    "- 역전파는 학습 과정의 매 단계마다 달라짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "- `torch.Tensor` 클래스의 `.requires_grad`속성을 `True`로 설정하면, 해당 텐서에서 이루어진 모든 연산을 추적(track)하기 시작\n",
    "- 계산이 완료된 후 `.backward()`를 호출하여 모든 변화도(gradient)를 자동으로 계산할 수 있으며 이 Tensor의 변화도(gradient)는 `.grad` 속성에 누적됨\n",
    "- Tensor가 기록을 추적하는 걸 중단하려면 `.detach()`를 호출하여 연산기록으로부터 분리하여 연산이 추적되는 것을 방지할 수 있음\n",
    "- 기록을 추적하는 것(과 메모리를 사용하는 것)을 방지하기 위해서 코드 블럭을 `with.torch.no_grad():`로 감쌀 수 있음\n",
    "- 이는 변화도는(gradient)는 필요없지만 `requires_grad=True`가 설정되어 학습 가능한 매개변수를 갖는 모델을 평가(evaluate)할 때 유용\n",
    "- Autograd 구현에서 `Function` 클래스는 매우 중요한 역할을 수행\n",
    "- `Tensor`와 `Function`은 서로 연결되어 있고 모든 연산과정을 부호화하여 순환하지 않는 그래프 생성\n",
    "- 각 Tensor는 `.grad_fn`속성을 가지고 있는데 이는 `Tensor`를 생성한 `Function`을 참조(단, 사용자가 만든 Tensor는 예외이며, 사용자가 만든 Tensor가 아닌 연산에 의해 생긴 텐서와 같은 경우는 모두 `Function`을 참조)\n",
    "- 도함수를 계산하기 위해서는 `Tensor`의 `.backward()` 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# x의 연산 과적을 추적하기 위해 requires_grad=True로 설정\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "# 직접 생선한 Tensor이기 때문에 grad_fn이 None인 것을 확인할 수 있음\n",
    "#grad_fn속성 : Tensor를 생성한 Function 참조\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# y, z, out은 연산의 결과로 생성된 것이기 때문에 grad_fn을 갖고 있는 것을 확인 가능\n",
    "y = x + 2\n",
    "y.retain_grad()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x000002848D0C3FD0>\n"
     ]
    }
   ],
   "source": [
    "# 연산의 결과로 생성된 것이기 때문에 grad_fn을 갖는 것을 확인 가능\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "# 각각 사용한 func에 맞게 grad_fn이 생성된 것을 확인할 수 있음\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `requires_grad_()`를 사용하면 기존 Tensor의 `requires_grad`값을 바꿀 수 있음\n",
    "- 입력 값이 지정되지 않으면 기본 값은 `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0235,  0.0437],\n",
      "        [-1.4578, -0.6392]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(2,2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[130.5737,  -0.1370],\n",
      "        [  1.7794,   1.1698]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = ((a * 3) / (a - 1))\n",
    "print(a)\n",
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[130.5737,  -0.1370],\n",
       "        [  1.7794,   1.1698]], requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#requires_grad_()를 사용하면 requires_grad값을 바꿀 수 있음\n",
    "a.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17054.0449, grad_fn=<SumBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#속성을 상속받음\n",
    "b=(a*a).sum()\n",
    "print(b)\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변화도(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)\n",
    "\n",
    "#이전에 만든 out을 사용해서 역전파 진행\n",
    "# 여러 번 역전파를 하기 위해서는 retain_graph=True로 지정\n",
    "out.backward(retain_graph=True)\n",
    "# out.backward(torch.tensor(1.))을 진행하는 것과 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역전파를 진행한 후의 x값\n",
    "# d(out) / dx\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이전에 만든 out을 사용해서 역전파 진행\n",
    "\n",
    "#retain_grad() : leaf가 아닌 Tensor의 .grad속성 활성화\n",
    "#중간값에 대한 미분값을 보고 싶다면 해당 값에 대한 retain_grad()호출해야함\n",
    "y.retain_grad()\n",
    "x.retain_grad()\n",
    "z.retain_grad()\n",
    "#backward() : 모든 변화도(gradient)를 자동으로 계산\n",
    "#여러 번 미분을 진행하기 위해서는 retain_graph=True로 설정해줘야함(안 하면 에러)\n",
    "out.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n",
      "False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\vscodeㅔㅛ쇄ㅜ\\7주차\\Pytorch tutorial(Autograd, MLP).ipynb 셀 17\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/vscode%E3%85%94%E3%85%9B%EC%87%84%E3%85%9C/7%EC%A3%BC%EC%B0%A8/Pytorch%20tutorial%28Autograd%2C%20MLP%29.ipynb#Y123sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(z\u001b[39m.\u001b[39mgrad)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/vscode%E3%85%94%E3%85%9B%EC%87%84%E3%85%9C/7%EC%A3%BC%EC%B0%A8/Pytorch%20tutorial%28Autograd%2C%20MLP%29.ipynb#Y123sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(z\u001b[39m.\u001b[39mis_leaf)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/vscode%E3%85%94%E3%85%9B%EC%87%84%E3%85%9C/7%EC%A3%BC%EC%B0%A8/Pytorch%20tutorial%28Autograd%2C%20MLP%29.ipynb#Y123sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m out\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/vscode%E3%85%94%E3%85%9B%EC%87%84%E3%85%9C/7%EC%A3%BC%EC%B0%A8/Pytorch%20tutorial%28Autograd%2C%20MLP%29.ipynb#Y123sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mgrad)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/vscode%E3%85%94%E3%85%9B%EC%87%84%E3%85%9C/7%EC%A3%BC%EC%B0%A8/Pytorch%20tutorial%28Autograd%2C%20MLP%29.ipynb#Y123sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[1;32mc:\\Users\\a0105\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\a0105\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "\n",
    "#이전에 만든 out을 사용해서 역전파 진행\n",
    "\n",
    "y.retain_grad()     #중간값에 대한 미분값을 보고 싶다면 해당 값에 대한 retain_grad()호출해야함\n",
    "z.retain_grad()\n",
    "out.backward()      #여러 번 미분을 진행하기 위해서는 retain_graph=True로 설정해줘야함(안 하면 에러)\n",
    "\n",
    "# out.backward(torch.tensor(1.))을 진행하는 것과 동일\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)\n",
    "print(z.is_leaf)\n",
    "\n",
    "out.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)  # 요소값이 1인 2x2행렬을 선언한다.\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(out)\n",
    "\n",
    "y.retain_grad()\n",
    "out.backward(retain_graph=True)\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)       # z.retain_grad()를 호출하지 않으면 grad값을 저장하지 않기 때문에 grad 속성을 볼 수 있음\n",
    "print(z.is_leaf)\n",
    "\n",
    "out.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 `torch.autograd`는 벡터-야코비안 곱을 계산하는 엔진\n",
    "- `torch.autograd`를 사용하면 전체 야코비안을 직접 계산할 수는 없지만, 벡터-야코비안 곱은 `backward`에 해당 벡터를 인자로 제공하여 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7879, -2.0423, -0.7464], requires_grad=True)\n",
      "tensor([-1.5758, -4.0846, -1.4928], grad_fn=<MulBackward0>)\n",
      "tensor([ -403.4163, -1045.6548,  -382.1657], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x * 2\n",
    "print(y)\n",
    "\n",
    "#.data.norm() : L2 norm(Frobenius norm)\n",
    "#=sqrt(sum(y의 모든 요소^2))\n",
    "while y.data.norm() < 1000:\n",
    "  y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "# scalar 값이 아닌 y의 벡터-야코비안 곱을 구하는 과정\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `with torch.no_grad()`로 코드 블록을 감싸서 `autograd`가 `.requires_grad=True`인 Tensor의 연산 기록을 추적하는 것을 멈출 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tprint(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 또는 .detach()를 호출하여 내용물은 같지만 requires_grad가 다른 새로운 Tensor를 생성할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.requires_grad)\n",
    "y=x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN(Artificial Neural Networks)\n",
    "- 신경망은 `torch.nn`패키지를 사용하여 생성가능\n",
    "- nn은 모델을 정의하고 미분하기 위해서 위에서 살펴본 `autograd`를 사용\n",
    "- `nn.Module`은 layer와 `output`을 반환하는 `forward(input)`메소드 포함\n",
    "예제)\n",
    "\n",
    "- 간단한 순전파 네트워크(feed-forward-network)\n",
    "- 입력을 받아 여러 계층(layer)에 차례로 전달 후 최종 출력 제공\n",
    "- 신경망의 일반적인 학습 과정\n",
    "\n",
    "1. 학습 가능한 매개변수(가중치)를 갖는 신경망 정의\n",
    "2. 데이터 셋 입력 반복\n",
    "3. 입력을 신경망에서 전파(process)\n",
    "4. 손실(loss, 입력값-예측값)를 계산\n",
    "5. 변화도(gradient)를 신경망의 매개변수들에 의해 역으로 전파(역전파)\n",
    "6. 신경망의 가중치 갱신\n",
    "- 새로운 가중치(weight)=가중치(weight)-학습률(learning rate)*변화도(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#라이브러리 import\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer0 = nn.Linear(4, 128)\n",
    "        self.layer1 = nn.Linear(128, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 16)\n",
    "        self.layer4 = nn.Linear(16, 3)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm1d(128)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.act(self.bn0(self.layer0(x)))\n",
    "      x = self.act(self.bn1(self.layer1(x)))\n",
    "      x = self.act(self.bn2(self.layer2(x)))\n",
    "      x = self.act(self.layer3(x))\n",
    "      x = self.layer4(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실함수(Loss function)\n",
    "- (output, target)을 한 쌍으로 입력받아 출력이 정답으로부터 얼마나 떨어져있는지 계산\n",
    "- `forward()`만 정의하면 `backward()`는 `autograd`에 의해 자동 정의\n",
    "- 모델의 학습 가능한 매개변수는 `net.parameters()`에 의해 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex_X :  tensor([[ 0.9859,  0.0335,  0.0845, -2.0727],\n",
      "        [-0.2647,  0.2539, -0.8186, -0.9914],\n",
      "        [ 0.2767, -0.8893, -0.3133, -1.4691],\n",
      "        [-2.3311, -0.3446, -0.2750,  1.3268]])\n",
      "ex_y :  tensor([1, 0, 2, 0])\n",
      "tensor([[-0.0179,  0.1466, -0.2009],\n",
      "        [-0.0852,  0.0872, -0.2477],\n",
      "        [ 0.0134,  0.3783, -0.3096],\n",
      "        [-0.2629,  0.3525,  0.0577]], grad_fn=<AddmmBackward0>)\n",
      "loss:  1.2414988279342651\n",
      "layer0.bias.grad before backward\n",
      "None\n",
      "True\n",
      "layer0.bias.grad after backward\n",
      "tensor([-0.1966,  0.1688,  0.0277])\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 값 생성\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "ex_X, ex_y = torch.randn([4, 4]), torch.tensor([1, 0, 2, 0])\n",
    "print('ex_X : ', ex_X)\n",
    "print('ex_y : ', ex_y)\n",
    "\n",
    "#모델 정의\n",
    "net = Net()\n",
    "#입력값을 모델로 훈련시킨 후 나온 출력값\n",
    "output = net(ex_X)\n",
    "print(output)\n",
    "#손실계산\n",
    "loss = criterion(output, ex_y)\n",
    "#계산한 손실 출력\n",
    "print('loss: ', loss.item())\n",
    "  \n",
    "net.zero_grad()\n",
    "\n",
    "print('layer0.bias.grad before backward')\n",
    "print(net.layer4.bias.grad)\n",
    "\n",
    "print(net.layer4.bias.is_leaf)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('layer0.bias.grad after backward')\n",
    "print(net.layer4.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "torch.Size([128, 4])\n"
     ]
    }
   ],
   "source": [
    "#net.parameters()에는 각각의 층들마다의 weight, bias값들이 들어있음\n",
    "#Linear Layer : 4, 128, 64, 32, 16, 3 -> 6개*2(wieght, bias) + BatchNorm1d Layer : 128, 64, 32->3개*2(weight, bias)\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())   # layer0의 weight\n",
    "\n",
    "# for i in range(len(params)):\n",
    "#   print(params[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 갱신\n",
    "가장 단순한 갱신 규칙 : 확률적 경사 하강법(SGD, Stochastic Gradient Descent)\n",
    "\n",
    "- 새로운 가중치(weight)=가중치(weight)-학습률(learning rate)*변화도(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.optim 패키지에 다양한 갱신 규칙이 규현되어 있음\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "output = net(ex_X)\n",
    "loss = criterion(output, ex_y)\n",
    "loss.backward()\n",
    "optimizer.step()  # 업데이트 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "#data load\n",
    "dataset = load_iris()\n",
    "\n",
    "data = dataset.data\n",
    "label = dataset.target\n",
    "\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data:  (150, 4)\n",
      "shape of label:  (150,)\n"
     ]
    }
   ],
   "source": [
    "print('shape of data: ', data.shape)\n",
    "print('shape of label: ', label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# 훈련과 테스트 데이터로 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.25)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25)\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "# numpy->Tensor\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "train_set = TensorDataset(X_train, y_train)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer0 = nn.Linear(4, 128)\n",
    "        self.layer1 = nn.Linear(128, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 16)\n",
    "        self.layer4 = nn.Linear(16, 3)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm1d(128)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.act(self.bn0(self.layer0(x)))\n",
    "      x = self.act(self.bn1(self.layer1(x)))\n",
    "      x = self.act(self.bn2(self.layer2(x)))\n",
    "      x = self.act(self.layer3(x))\n",
    "      x = self.layer4(x)\n",
    "\n",
    "      return x\n",
    "      # return nn.Softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = list()\n",
    "accuracies = list()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  epoch_loss = 0  \n",
    "  epoch_accuracy = 0\n",
    "  for X, y in train_loader:\n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = net(X)\n",
    "\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    # output = [0.11, 0.5, 0.8] --> 예측 클래스 값\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "    accuracy = (predicted == y).sum().item()\n",
    "    epoch_loss += loss.item()\n",
    "    epoch_accuracy += accuracy\n",
    "\n",
    "  epoch_loss /= len(train_loader)\n",
    "  epoch_accuracy /= len(X_train)\n",
    "  print(\"epoch :{}, \\tloss :{}, \\taccuracy :{}\".format(str(epoch+1).zfill(3),round(epoch_loss,4), round(epoch_accuracy,4)))\n",
    "  \n",
    "  losses.append(epoch_loss)\n",
    "  accuracies.append(epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"$loss$\",fontsize = 18)\n",
    "plt.plot(losses)\n",
    "plt.grid()\n",
    "plt.xlabel(\"$epochs$\", fontsize = 16)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"$accuracy$\", fontsize = 18)\n",
    "plt.plot(accuracies)\n",
    "plt.grid()\n",
    "plt.xlabel(\"$epochs$\", fontsize = 16)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "output = net(X_test)\n",
    "print(torch.max(output, dim=1))\n",
    "_, predicted = torch.max(output, dim=1)\n",
    "accuracy = round((predicted == y_test).sum().item() / len(y_test),4)\n",
    "\n",
    "print(\"test_set accuracy :\", round(accuracy,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow-Tutorial(GradientTape, MLP)\n",
    "그래디언트 테이프\n",
    "\n",
    "- 자동미분을 위한 `tf.GradientTape` API제공\n",
    "- `tf.GradientTape`는 모든 연산을 테이프에 기록\n",
    "- 후진 방식 자동 미분(reverse mode differetiation)을 사용해서 테이프에 기록된 연산 그래디언트 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((2, 2))\n",
    "# 1, 1\n",
    "# 1, 1\n",
    "with tf.GradientTape() as t:\n",
    "  t.watch(x)\n",
    "  #reduce_sum() : 특정 축을 기준으로 합을 구해줌, 아무것도 입력 안 하면 모든 요소의 합\n",
    "  y = tf.reduce_sum(x)\n",
    "  print('y: ', y)\n",
    "  z = tf.multiply(y, y)\n",
    "  print('z: ', z)\n",
    "\n",
    "# 입력 텐서 x에 대한 z의 도함수\n",
    "dz_dx = t.gradient(z, x)\n",
    "print(dz_dx)\n",
    "\n",
    "for i in [0, 1]:\n",
    "  for j in [0, 1]:\n",
    "    # dz_dx[i][j]가 8이 아니면 AssertionError\n",
    "    assert dz_dx[i][j].numpy() == 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((2, 2))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    print('y: ', y)\n",
    "    z = tf.multiply(y,y)\n",
    "    print('z: ', z)\n",
    "    \n",
    "# 입력 텐서 x에 대한 z의 도함수\n",
    "dz_dx = t.gradient(z, x)\n",
    "print(dz_dx)\n",
    "for i in [0, 1]:\n",
    "    for j in [0, 1]:\n",
    "        # AssertionError가 발생하지 앟음\n",
    "        assert dz_dx[i][j].numpy() == 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((2, 2))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y,y)\n",
    "    \n",
    "# tf.GradientTape() 안에서 계산된 중간 값에 대한 그래디언트도 구할 수 있습니다.\n",
    "# 테이프 사용하여 중간값 y에 대한 도함수를 계산합니다.\n",
    "dz_dy = t.gradient(z, y)\n",
    "assert dz_dy.numpy() == 8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `GradientTape.gradient()` 메소드가 호출되면 GradientTape에 포함된 리소스가 해제\n",
    "- 동일한 연산에 대해 여러 gradient를 계산하려면 지속성있는(`persistent=True`) 그래디언트 테이프 생성하면됨\n",
    "- 이렇게 생성한 그래디언트 테이프는 `gradient()`메소드의 다중 호출 허용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "print(x)\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "  t.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y.  # z = x ^ 4\n",
    "dz_dx = t.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n",
    "print(dz_dx)\n",
    "dy_dx = t.gradient(y, x)  # 6.0   (2 * x at x = 3)\n",
    "print(dy_dx)\n",
    "del t  # 테이프에 대한 참조를 삭제합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제어 흐름 기록\n",
    "- 연산이 실행되는 순서대로 테이프에 기록되기 때문에, 파이썬 제어흐름이 자연스럽게 처리됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "  output = 1.0\n",
    "  for i in range(y):\n",
    "    if i > 1 and i < 5:   # output(1) * 2 * 3 * 4    #i가 2,3,4일 때 output*=x output은 x^n승\n",
    "      output = tf.multiply(output, x)\n",
    "  return output\n",
    "\n",
    "def grad(x, y):\n",
    "  with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    out = f(x, y)\n",
    "  return t.gradient(out, x)\n",
    "\n",
    "x = tf.convert_to_tensor(2.0)\n",
    "print(x)\n",
    "\n",
    "print(grad(x, 6).numpy())\t#output=2^3->gradient=3*2^2\n",
    "assert grad(x, 6).numpy() == 12.0\n",
    "\n",
    "print(grad(x, 5).numpy())\t#output=2^3->gradient=3*2^2\n",
    "assert grad(x, 5).numpy() == 12.0\n",
    "\n",
    "print(grad(x, 4).numpy()) \t#output=2^2->gradient=2*2^1\n",
    "assert grad(x, 4).numpy() == 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 고계도(Higher-order) 그래디언트\n",
    "- `GradientTape` 컨텍스트 매이저 안에 있는 연산들은 자동미분을 위해 기록됨\n",
    "- 만약 이 컨텍스트 안에서 그래디언트를 계산하면 해당 그래디언트 연산 또한 기록됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(1.0) \n",
    "print(x)\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "  with tf.GradientTape() as t2:\n",
    "    y = x * x * x\n",
    "    # 't' 컨텍스트 매니저 안의 그래디언트를 계산합니다.\n",
    "    # 이것은 또한 그래디언트 연산 자체도 미분가능하다는 것을 의미합니다.\n",
    "  dy_dx = t2.gradient(y, x)      # dy_dx = 3 * x^2 at x = 1\n",
    "d2y_dx2 = t.gradient(dy_dx, x)   # d2y_dx2 = 6 * x at x = 1\n",
    "\n",
    "assert dy_dx.numpy() == 3.0\n",
    "assert d2y_dx2.numpy() == 6.0\n",
    "print(dy_dx)\n",
    "print(d2y_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN(Artificial Neural Network)\n",
    "Sequential 모델을 사용하는 경우\n",
    "\n",
    "- `Seuquential` 모델은 각 레이어에 정확히 하나의 입력 Tensor와 하나의 출력 Tensor가 있는 일반 레이어 스택에 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sequential model with 3 layers\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2, activation=\"relu\", name=\"layer1\"),  # Pytorch - nn.Linear \n",
    "        layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n",
    "        layers.Dense(4, name=\"layer3\"),\n",
    "    ]\n",
    ")\n",
    "# Call model on a test input\n",
    "x = tf.ones((3, 3))\n",
    "# [1, 1, 1] --> [o, o] --> [o, o, o] --> [o, o, o, o]\n",
    "y = model(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input --> model1 --> model2 --> output\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2, activation=\"relu\", name=\"layer1\"),  # Pytorch - nn.Linear\n",
    "        # dense --> act:relu\n",
    "        layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n",
    "        # dense --> act:relu\n",
    "        layers.Dense(4, name=\"layers3\"),\n",
    "        # dense\n",
    "    ]\n",
    "    \n",
    "input\n",
    "model1 = model\n",
    "model2 = model\n",
    "output\n",
    "\n",
    "input\n",
    "dense(2)\n",
    "dense(3)\n",
    "dense(4)\n",
    "dense(2)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 layers\n",
    "# 위의 함수와 동일\n",
    "# Sequential 함수를 사용하지 않고 쌓을 경우\n",
    "layer1 = layers.Dense(2, activation=\"relu\", name=\"layer1\")\n",
    "layer2 = layers.Dense(3, activation=\"relu\", name=\"layer2\")\n",
    "layer3 = layers.Dense(4, name=\"layer3\")\n",
    "\n",
    "# Call layers on a test input\n",
    "x = tf.ones((3, 3))\n",
    "y = layer3(layer2(layer1(x)))\n",
    "print(y)    # y = w * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers 속성을 사용해서 레이어에 대해 접근할 수 있음\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add() 메서드를 통해서 Sequential 모델을 점진적으로 작성할 수도 있음\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(2, activation=\"relu\"))\n",
    "model.add(layers.Dense(3, activation=\"relu\"))\n",
    "model.add(layers.Dense(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop() 메서드를 사용하면 레이어를 제거할 수 있음\n",
    "model.pop()\n",
    "print(len(model.layers))    #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패션 MNIST를 사용한 분류 문제\n",
    "10개의 카테고리, 70000개의 흑백이미지(28*28)\n",
    "\n",
    "훈련에 60000개, 평가에 10000개 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow와 tf.keras를 임포트합니다\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# 헬퍼(helper) 라이브러리를 임포트합니다\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 모델에 주입하기 전에 값의 범위를 0~1로 조정\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', # SGD, SGD + momentum\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련된 모델을 사용하여 이미지에 대한 예측 만들기\n",
    "predictions = model.predict(test_images)\n",
    "# 테스트 세트에 있는 각 이미지에 대한 예측을 진행한 후, 첫번째 예측 값\n",
    "# 10개의 옷 품목에 상응하는 모델의 신뢰도(confidence)를 나타냄\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 높은 신뢰도를 가진 레이블 출력\n",
    "print(np.argmax(predictions[0]))\n",
    "# 실제 테스트 데이터의 0번째 값\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10개의 클래스에 대한 예측을 모두 그래프로 표현\n",
    "# 올바르게 예측된 레이블은 파란색으로, 잘못 예측된 레이블은 빨강색으로 표현\n",
    "# 숫자는 예측 레이블의 신뢰도 퍼센트\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "\n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "  predictions_array, true_label = predictions_array[i], true_label[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1])\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음 X 개의 테스트 이미지와 예측 레이블, 진짜 레이블을 출력합니다\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions, test_labels, test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions, test_labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d1b64f0a4f75ff6fce310d5cdee975b7f3ac36b18d89ae5a26ef7cac4af149c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
