{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ye-seul2/cnu_sw_academy/blob/main/Attention_is_All_You_Need_Tutorial__Korean_English_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfTcLf-rjaWd"
      },
      "source": [
        "#### **Attention is All You Need (NIPS 2017)** 실습\n",
        "* <b>(뉴스 데이터셋)</b> 한국어 문장을 영어 문장으로 번역합니다.\n",
        "* 본 코드는 기본적으로 **Transformer** 논문의 내용을 최대한 따릅니다.\n",
        "    * 본 논문은 **딥러닝 기반의 자연어 처리** 기법의 기본적인 구성을 이해하고 공부하는 데에 도움을 줍니다.\n",
        "    * 2020년 기준 가장 뛰어난 번역 모델들은 본 논문에서 제안한 **Transformer 기반의 아키텍처**를 따르고 있습니다.\n",
        "* 코드 실행 전에 **[런타임]** → **[런타임 유형 변경]** → 유형을 **GPU**로 설정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXXXOxNzeLpj"
      },
      "source": [
        "#### <b>한글 출력을 위한 폰트 설치</b>\n",
        "\n",
        "* 설치 이후에 수동으로 <b>[런타임]</b> - <b>[런타임 다시 시작]</b> 버튼을 눌러 재시작합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65nhszH6eNPt",
        "outputId": "e371975e-790d-4d1d-c3f3-01241b3838ab"
      },
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 9,604 kB of archives.\n",
            "After this operation, 29.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB]\n",
            "Fetched 9,604 kB in 1s (9,192 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123942 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20170925-1_all.deb ...\n",
            "Unpacking fonts-nanum (20170925-1) ...\n",
            "Setting up fonts-nanum (20170925-1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epJDFH3k03M6"
      },
      "source": [
        "#### <b>BLEU Score 계산을 위한 라이브러리 업데이트</b>\n",
        "\n",
        "* <b>[Restart Runtime]</b> 버튼을 눌러 런타임을 재시작할 필요가 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrr-S31b031u",
        "outputId": "d28d6220-337b-4757-cedf-993e7cf14124"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.21.6)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 37.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (4.1.1)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "Successfully installed sentencepiece-0.1.97 torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OdH8Empjyxa"
      },
      "source": [
        "#### <b>한글 토큰화 라이브러리 설치하기</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2O0iSKl2mT_",
        "outputId": "db442deb-ed38-4b7c-aff1-65e493cbcd6f"
      },
      "source": [
        "!pip3 install konlpy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 56.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5h73Lh9j2GW"
      },
      "source": [
        "#### <b>데이터셋 다운로드</b>\n",
        "\n",
        "* 한영 번역 데이터셋을 다운로드하여 파이썬 객체로 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdPZWIgl2oeF",
        "outputId": "addc8f0c-db69-4596-bbca-ed662b90d035"
      },
      "source": [
        "# 한영 번역 데이터셋을 포함하는 저장소\n",
        "!git clone https://github.com/ndb796/korean-parallel-corpora"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'korean-parallel-corpora'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Total 131 (delta 0), reused 0 (delta 0), pack-reused 131\u001b[K\n",
            "Receiving objects: 100% (131/131), 17.67 MiB | 13.62 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2iBk4-k2qR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a44a89b-ad00-4d4b-c53c-56a818a981dc"
      },
      "source": [
        "# 데이터셋이 저장될 폴더 생성\n",
        "!mkdir -p ./dataset\n",
        "\n",
        "# 압축 해제\n",
        "!tar -xvf ./korean-parallel-corpora/korean-english-news-v1/korean-english-park.train.tar.gz -C ./dataset\n",
        "!tar -xvf ./korean-parallel-corpora/korean-english-news-v1/korean-english-park.test.tar.gz -C ./dataset\n",
        "!tar -xvf ./korean-parallel-corpora/korean-english-news-v1/korean-english-park.dev.tar.gz -C ./dataset\n",
        "\n",
        "# 학습(training) 데이터셋 이름 변경\n",
        "!mv ./dataset/korean-english-park.train.en ./dataset/train.en\n",
        "!mv ./dataset/korean-english-park.train.ko ./dataset/train.ko\n",
        "\n",
        "# 평가(validation) 데이터셋 이름 변경\n",
        "!mv ./dataset/korean-english-park.dev.en ./dataset/dev.en\n",
        "!mv ./dataset/korean-english-park.dev.ko ./dataset/dev.ko\n",
        "\n",
        "# 테스트(test) 데이터셋 이름 변경\n",
        "!mv ./dataset/korean-english-park.test.en ./dataset/test.en\n",
        "!mv ./dataset/korean-english-park.test.ko ./dataset/test.ko"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "korean-english-park.train.en\n",
            "korean-english-park.train.ko\n",
            "korean-english-park.test.en\n",
            "korean-english-park.test.ko\n",
            "korean-english-park.dev.en\n",
            "korean-english-park.dev.ko\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_JbLlFqOpAo"
      },
      "source": [
        "#### <b>데이터셋 읽어 확인하기</b>\n",
        "\n",
        "* 학습, 평가, 테스트 데이터셋을 각각 읽어 문장 데이터를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcTMM_Em2rUJ"
      },
      "source": [
        "korean_lines_train = open(\"./dataset/train.ko\", 'r', encoding='utf-8').readlines()\n",
        "english_lines_train = open(\"./dataset/train.en\", 'r', encoding='utf-8').readlines()\n",
        "\n",
        "korean_lines_val = open(\"./dataset/dev.ko\", 'r', encoding='utf-8').readlines()\n",
        "english_lines_val = open(\"./dataset/dev.en\", 'r', encoding='utf-8').readlines()\n",
        "\n",
        "korean_lines_test = open(\"./dataset/test.ko\", 'r', encoding='utf-8').readlines()\n",
        "english_lines_test = open(\"./dataset/test.en\", 'r', encoding='utf-8').readlines()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np7PF8SFkBNv",
        "outputId": "3def5bad-8cd6-4577-f08c-c3d18eaaa21d"
      },
      "source": [
        "print(f\"한글 문장 학습 데이터 개수: {len(korean_lines_train)}개\")\n",
        "print(f\"영어 문장 학습 데이터 개수: {len(english_lines_train)}개\")\n",
        "\n",
        "print(f\"한글 문장 평가 데이터 개수: {len(korean_lines_val)}개\")\n",
        "print(f\"영어 문장 평가 데이터 개수: {len(english_lines_val)}개\")\n",
        "\n",
        "print(f\"한글 문장 테스트 데이터 개수: {len(korean_lines_test)}개\")\n",
        "print(f\"영어 문장 테스트 데이터 개수: {len(english_lines_test)}개\")\n",
        "\n",
        "index = 777\n",
        "print(f\"{index + 1}번째 학습용 한글 문장:\", korean_lines_train[index], end='')\n",
        "print(f\"{index + 1}번째 학습용 영어 문장:\", english_lines_train[index], end='')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한글 문장 학습 데이터 개수: 94123개\n",
            "영어 문장 학습 데이터 개수: 94123개\n",
            "한글 문장 평가 데이터 개수: 1000개\n",
            "영어 문장 평가 데이터 개수: 1000개\n",
            "한글 문장 테스트 데이터 개수: 2000개\n",
            "영어 문장 테스트 데이터 개수: 2000개\n",
            "778번째 학습용 한글 문장: 지금 21살인 유는 학교에 가기 전 서너시간 동안 컴퓨터 통신에 끼어들기 위해 새벽 5시에 침대에서 일어나 나온다.\n",
            "778번째 학습용 영어 문장: Now Yu, 21, drags herself out of bed at 5 a.m. to squeeze in a few hours online before school.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcLKB3BRkDKW"
      },
      "source": [
        "#### <b>단어 사전 만들기 </b>\n",
        "\n",
        "* 단어 사전 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWpaBry12tjd"
      },
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self):\n",
        "        self.UNK = '<unk>'\n",
        "        self.PAD = '<pad>'\n",
        "        self.SOS = '<sos>'\n",
        "        self.EOS = '<eos>'\n",
        "\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.word2count = {}\n",
        "\n",
        "    # 하나의 문장(sentence)에 포함된 모든 토큰을 추가하는 함수\n",
        "    def add_tokens(self, tokens):\n",
        "        for word in tokens:\n",
        "            if word in self.word2count:\n",
        "                self.word2count[word] += 1\n",
        "            else:\n",
        "                self.word2count[word] = 1\n",
        "\n",
        "    def preprocess(self, min_count):\n",
        "        # 사용하지 않을 단어 집합\n",
        "        trim_words = set()\n",
        "        for word, count in self.word2count.items():\n",
        "            if count < min_count:\n",
        "                trim_words.add(word)\n",
        "\n",
        "        # 실제로 사용할 단어만 남기기\n",
        "        words = set(self.word2count.keys()) - trim_words\n",
        "        words = [self.UNK, self.PAD, self.SOS, self.EOS] + list(words)\n",
        "\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        for i, word in enumerate(words):\n",
        "            self.word2idx[word] = i\n",
        "            self.idx2word[i] = word"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSB5vFULkecQ"
      },
      "source": [
        "#### <b>문장 토큰화</b>\n",
        "\n",
        "* 먼저 한글 문장 및 영어 문장 데이터셋에 대하여 토큰화를 수행합니다.\n",
        "* 토큰화를 위해 특수문자 제거 함수를 정의하고 객체를 초기화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDH1f2gqlxO-"
      },
      "source": [
        "import re\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "def clean_string(string):\n",
        "    string = string.strip() # 앞뒤로 존재하는 공백 제거\n",
        "    string = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', string) # 특수문자 제거\n",
        "    return string.strip().lower() # 소문자로 변환하여 반환\n",
        "\n",
        "okt = Okt() # 한글 형태소 분석기"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fP80_AZmSC8"
      },
      "source": [
        "* 학습(training) 데이터셋을 토큰화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyKq4u8pmRlM",
        "outputId": "47325c43-ff13-40b8-b7aa-9fae6816145e"
      },
      "source": [
        "tokenized_korean_lines_train = []\n",
        "tokenized_english_lines_train = []\n",
        "\n",
        "min_length = 4 # 단어의 개수가 4개 이상인 학습 문장 쌍만 사용\n",
        "max_length = 50 # 단어의 개수가 50개 이하인 학습 문장 쌍만 사용\n",
        "\n",
        "for i in range(len(korean_lines_train)):\n",
        "    korean = korean_lines_train[i]\n",
        "    korean = clean_string(korean)\n",
        "    korean_tokens = [line[0] for line in okt.pos(korean, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "    english = english_lines_train[i]\n",
        "    english = clean_string(english)\n",
        "    english_tokens = english.split(' ')\n",
        "\n",
        "    if len(korean_tokens) < min_length or len(korean_tokens) > max_length:\n",
        "        continue\n",
        "    if len(english_tokens) < min_length or len(english_tokens) > max_length:\n",
        "        continue\n",
        "\n",
        "    tokenized_korean_lines_train.append(korean_tokens)\n",
        "    tokenized_english_lines_train.append(english_tokens)\n",
        "\n",
        "    if (i + 1) % 4000 == 0:\n",
        "        print(f\"학습 데이터셋 토큰화: {i + 1}/{len(korean_lines_train)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터셋 토큰화: 4000/94123\n",
            "학습 데이터셋 토큰화: 8000/94123\n",
            "학습 데이터셋 토큰화: 12000/94123\n",
            "학습 데이터셋 토큰화: 16000/94123\n",
            "학습 데이터셋 토큰화: 20000/94123\n",
            "학습 데이터셋 토큰화: 24000/94123\n",
            "학습 데이터셋 토큰화: 28000/94123\n",
            "학습 데이터셋 토큰화: 32000/94123\n",
            "학습 데이터셋 토큰화: 36000/94123\n",
            "학습 데이터셋 토큰화: 40000/94123\n",
            "학습 데이터셋 토큰화: 44000/94123\n",
            "학습 데이터셋 토큰화: 48000/94123\n",
            "학습 데이터셋 토큰화: 52000/94123\n",
            "학습 데이터셋 토큰화: 56000/94123\n",
            "학습 데이터셋 토큰화: 60000/94123\n",
            "학습 데이터셋 토큰화: 64000/94123\n",
            "학습 데이터셋 토큰화: 68000/94123\n",
            "학습 데이터셋 토큰화: 72000/94123\n",
            "학습 데이터셋 토큰화: 76000/94123\n",
            "학습 데이터셋 토큰화: 80000/94123\n",
            "학습 데이터셋 토큰화: 84000/94123\n",
            "학습 데이터셋 토큰화: 88000/94123\n",
            "학습 데이터셋 토큰화: 92000/94123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DerYg_C1nOk7"
      },
      "source": [
        "* 평가(validation) 데이터셋을 토큰화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKrnkrbTnPoc",
        "outputId": "1ef4bd70-5650-4cea-db17-481cba757057"
      },
      "source": [
        "tokenized_korean_lines_val = []\n",
        "tokenized_english_lines_val = []\n",
        "\n",
        "for i in range(len(korean_lines_val)):\n",
        "    korean = korean_lines_val[i]\n",
        "    korean = clean_string(korean)\n",
        "    korean_tokens = [line[0] for line in okt.pos(korean, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "    english = english_lines_val[i]\n",
        "    english = clean_string(english)\n",
        "    english_tokens = english.split(' ')\n",
        "\n",
        "    tokenized_korean_lines_val.append(korean_tokens)\n",
        "    tokenized_english_lines_val.append(english_tokens)\n",
        "\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"평가 데이터셋 토큰화: {i + 1}/{len(korean_lines_val)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "평가 데이터셋 토큰화: 1000/1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_sP1Wn3pvQU"
      },
      "source": [
        "* 테스트(test) 데이터셋을 토큰화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxY4lieapyQj",
        "outputId": "ee35d3ae-07ae-4bb3-b64e-7ef5ed5cde5a"
      },
      "source": [
        "tokenized_korean_lines_test = []\n",
        "tokenized_english_lines_test = []\n",
        "\n",
        "for i in range(len(korean_lines_test)):\n",
        "    korean = korean_lines_test[i]\n",
        "    korean = clean_string(korean)\n",
        "    korean_tokens = [line[0] for line in okt.pos(korean, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "    english = english_lines_test[i]\n",
        "    english = clean_string(english)\n",
        "    english_tokens = english.split(' ')\n",
        "\n",
        "    tokenized_korean_lines_test.append(korean_tokens)\n",
        "    tokenized_english_lines_test.append(english_tokens)\n",
        "\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"테스트 데이터셋 토큰화: {i + 1}/{len(korean_lines_test)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 데이터셋 토큰화: 1000/2000\n",
            "테스트 데이터셋 토큰화: 2000/2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxvJN5THq2hx"
      },
      "source": [
        "#### <b>단어 사전 만들기</b>\n",
        "\n",
        "* 최소 2번 이상 등장한 단어만 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNVKZ7NbrP2l",
        "outputId": "45361edf-e6b1-4813-c9e4-12603b624098"
      },
      "source": [
        "korean_voca = Vocabulary()\n",
        "english_voca = Vocabulary()\n",
        "\n",
        "for i in range(len(tokenized_korean_lines_train)):\n",
        "    korean_tokens = tokenized_korean_lines_train[i]\n",
        "    english_tokens = tokenized_english_lines_train[i]\n",
        "\n",
        "    korean_voca.add_tokens(korean_tokens)\n",
        "    english_voca.add_tokens(english_tokens)\n",
        "\n",
        "korean_voca.preprocess(min_count=2)\n",
        "english_voca.preprocess(min_count=2)\n",
        "\n",
        "print(\"전체 한국어 단어 수:\", len(korean_voca.word2count))\n",
        "print(\"전체 영어 단어 수:\", len(english_voca.word2count))\n",
        "print(\"사용할 한국어 토큰 수:\", len(korean_voca.word2idx))\n",
        "print(\"사용할 영어 토큰 수:\", len(english_voca.word2idx))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 한국어 단어 수: 67029\n",
            "전체 영어 단어 수: 58833\n",
            "사용할 한국어 토큰 수: 40612\n",
            "사용할 영어 토큰 수: 35745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wyKQjwAusyw",
        "outputId": "c236acd6-93ac-4ac3-e8e1-812cb6a66778"
      },
      "source": [
        "print(korean_voca.word2idx['<pad>']) # 패딩(padding): 1\n",
        "print(korean_voca.word2idx['<sos>']) # <sos>: 2\n",
        "print(korean_voca.word2idx['<eos>']) # <eos>: 3\n",
        "print(korean_voca.word2idx['컴퓨터'])\n",
        "print(korean_voca.word2idx['사랑'])\n",
        "print(korean_voca.word2idx['기적'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "25454\n",
            "23675\n",
            "19579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQjD1bjVuvHt",
        "outputId": "b8b40095-3938-4f42-e216-685b397348cf"
      },
      "source": [
        "print(english_voca.word2idx['<pad>']) # 패딩(padding): 1\n",
        "print(english_voca.word2idx['<sos>']) # <sos>: 2\n",
        "print(english_voca.word2idx['<eos>']) # <eos>: 3\n",
        "print(english_voca.word2idx['computer'])\n",
        "print(english_voca.word2idx['love'])\n",
        "print(english_voca.word2idx['miracle'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "16097\n",
            "19849\n",
            "19176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw7jo7dmsHRW"
      },
      "source": [
        "* Unknown Token이 1개 이상 포함된 문장은 데이터셋에서 제외하여 다시 학습 데이터셋을 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M717ROWBsFmM"
      },
      "source": [
        "unknown_threshold = 1\n",
        "\n",
        "preprocessed_korean_lines_train = []\n",
        "preprocessed_english_lines_train = []\n",
        "\n",
        "for i in range(len(tokenized_korean_lines_train)):\n",
        "    korean_tokens = tokenized_korean_lines_train[i]\n",
        "    english_tokens = tokenized_english_lines_train[i]\n",
        "\n",
        "    is_used = True # 현재의 문장 쌍을 사용할지의 여부\n",
        "    for token in korean_tokens:\n",
        "        cnt = 0\n",
        "        if token not in korean_voca.word2idx:\n",
        "            cnt += 1\n",
        "        if cnt >= unknown_threshold:\n",
        "            is_used = False\n",
        "    for token in english_tokens:\n",
        "        cnt = 0\n",
        "        if token not in english_voca.word2idx:\n",
        "            cnt += 1\n",
        "        if cnt >= unknown_threshold:\n",
        "            is_used = False\n",
        "\n",
        "    if not is_used:\n",
        "        continue\n",
        "\n",
        "    preprocessed_korean_lines_train.append(korean_tokens)\n",
        "    preprocessed_english_lines_train.append(english_tokens)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg6lc1V4uDQ6",
        "outputId": "5d2b38e5-1e5d-4010-cd75-ad42c32941f2"
      },
      "source": [
        "print(\"사용할 한국어 학습 문장 수:\", len(preprocessed_korean_lines_train))\n",
        "print(\"사용할 영어 학습 문장 수:\", len(preprocessed_english_lines_train))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용할 한국어 학습 문장 수: 61029\n",
            "사용할 영어 학습 문장 수: 61029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH1SIu_i5mch",
        "outputId": "cc11c922-b97d-480f-e54e-6a4de7f6fc5f"
      },
      "source": [
        "print(preprocessed_korean_lines_train[7777])\n",
        "print(preprocessed_english_lines_train[7777])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cnn', '의', '여론조사', '국장', '인', '키팅', '홀랜드', '는', '“', '이라크전', '발발', '직후', '부시', '의', '지지도', '는', '71', '였다', '”', '며', '“', '지지율', '40', '추락', '은', '베트남전', '당시', '린', '든', '존슨', '대통령', '과', '유사하다', '”', '고', '지적', '했다']\n",
            "['bushs', 'approval', 'rating', 'five', 'years', 'ago', 'at', 'the', 'start', 'of', 'the', 'iraq', 'war', 'was', '71', 'percent', 'and', 'that', '40point', 'drop', 'is', 'almost', 'identical', 'to', 'the', 'drop', 'president', 'lyndon', 'johnson', 'faced', 'during', 'the', 'vietnam', 'war', 'said', 'cnn', 'polling', 'director', 'keating', 'holland']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eItQjid_uzsB"
      },
      "source": [
        "#### <b>커스텀 데이터셋 클래스 작성하기</b>\n",
        "\n",
        "* 소스 문장(한국어)과 타겟 문장(영어)를 한 쌍으로 반환하는 데이터셋 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C7JV5PE5rmM"
      },
      "source": [
        "import copy\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, korean_lines, english_lines, max_seq_len):\n",
        "        self.korean_lines = korean_lines\n",
        "        self.english_lines = english_lines\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoder_input = self.get_encoder_input(self.korean_lines[index])\n",
        "        decoder_input = self.get_decoder_input(self.english_lines[index])\n",
        "\n",
        "        return encoder_input, decoder_input\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.korean_lines)\n",
        "\n",
        "    # 한글 문장 벡터화\n",
        "    def get_encoder_input(self, tokens):\n",
        "        tokens = copy.deepcopy(tokens)\n",
        "        tokens.insert(0, korean_voca.SOS)\n",
        "        tokens.append(korean_voca.EOS)\n",
        "        tokens = self.padding(tokens, korean_voca) # 문장 뒤쪽에 패딩 붙이기\n",
        "        index_list = self.word2idx(tokens, korean_voca)\n",
        "\n",
        "        return torch.tensor(index_list).to(device)\n",
        "\n",
        "    # 영어 문장 벡터화\n",
        "    def get_decoder_input(self, tokens):\n",
        "        tokens = copy.deepcopy(tokens)\n",
        "        tokens.insert(0, english_voca.SOS)\n",
        "        tokens.append(english_voca.EOS)\n",
        "        tokens = self.padding(tokens, english_voca) # 문장 뒤쪽에 패딩 붙이기\n",
        "        index_list = self.word2idx(tokens, english_voca)\n",
        "\n",
        "        return torch.tensor(index_list).to(device)\n",
        "\n",
        "    # max_seq_len보다 길이가 짧은 문장에 대해 <pad> 토큰 채우기\n",
        "    def padding(self, tokens, voca):\n",
        "        if len(tokens) < self.max_seq_len:\n",
        "            tokens += [voca.PAD] * (self.max_seq_len - len(tokens))\n",
        "        else:\n",
        "            tokens = tokens[:self.max_seq_len]\n",
        "        return tokens\n",
        "\n",
        "    def word2idx(self, tokens, voca):\n",
        "        idx_list = []\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                idx_list.append(voca.word2idx[token])\n",
        "            except KeyError:\n",
        "                idx_list.append(voca.word2idx[voca.UNK])\n",
        "        return idx_list"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EObNRkSSvPfk"
      },
      "source": [
        "* 학습/평가/테스트 데이터셋 객체를 초기화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBsC-CRc5_F7"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_dataset = CustomDataset(preprocessed_korean_lines_train, preprocessed_english_lines_train, max_seq_len=80)\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=128, num_workers=0)\n",
        "\n",
        "val_dataset = CustomDataset(tokenized_korean_lines_val, tokenized_english_lines_val, max_seq_len=80)\n",
        "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=128, num_workers=0)\n",
        "\n",
        "test_dataset = CustomDataset(tokenized_korean_lines_test, tokenized_english_lines_test, max_seq_len=80)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=128, num_workers=0)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlKQSsxK6BdK",
        "outputId": "82e24c2e-cd3a-4b7e-cd6a-5d4759fc2fd0"
      },
      "source": [
        "# 하나의 배치에 포함되어 있는 문장을 출력합니다.\n",
        "for i, batch in enumerate(train_loader):\n",
        "    src = batch[0]\n",
        "    trg = batch[1]\n",
        "\n",
        "    print(f\"첫 번째 배치 크기: {src.shape}\")\n",
        "\n",
        "    # 현재 배치에 있는 하나의 문장에 포함된 정보 출력\n",
        "    for i in range(src.shape[1]):\n",
        "        print(f\"인덱스 {i}: {src[0][i].item()}\") # 여기에서는 [Seq_num, Seq_len]\n",
        "\n",
        "    # 첫 번째 배치만 확인\n",
        "    break"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 배치 크기: torch.Size([128, 80])\n",
            "인덱스 0: 2\n",
            "인덱스 1: 35362\n",
            "인덱스 2: 7276\n",
            "인덱스 3: 921\n",
            "인덱스 4: 28675\n",
            "인덱스 5: 23184\n",
            "인덱스 6: 10086\n",
            "인덱스 7: 31515\n",
            "인덱스 8: 22633\n",
            "인덱스 9: 16764\n",
            "인덱스 10: 17445\n",
            "인덱스 11: 15980\n",
            "인덱스 12: 4176\n",
            "인덱스 13: 31515\n",
            "인덱스 14: 24494\n",
            "인덱스 15: 17445\n",
            "인덱스 16: 27386\n",
            "인덱스 17: 26159\n",
            "인덱스 18: 25691\n",
            "인덱스 19: 25711\n",
            "인덱스 20: 859\n",
            "인덱스 21: 14412\n",
            "인덱스 22: 30938\n",
            "인덱스 23: 1052\n",
            "인덱스 24: 34470\n",
            "인덱스 25: 3316\n",
            "인덱스 26: 15117\n",
            "인덱스 27: 37055\n",
            "인덱스 28: 22931\n",
            "인덱스 29: 15760\n",
            "인덱스 30: 3084\n",
            "인덱스 31: 11386\n",
            "인덱스 32: 19217\n",
            "인덱스 33: 35982\n",
            "인덱스 34: 39680\n",
            "인덱스 35: 25719\n",
            "인덱스 36: 7216\n",
            "인덱스 37: 38756\n",
            "인덱스 38: 31633\n",
            "인덱스 39: 3\n",
            "인덱스 40: 1\n",
            "인덱스 41: 1\n",
            "인덱스 42: 1\n",
            "인덱스 43: 1\n",
            "인덱스 44: 1\n",
            "인덱스 45: 1\n",
            "인덱스 46: 1\n",
            "인덱스 47: 1\n",
            "인덱스 48: 1\n",
            "인덱스 49: 1\n",
            "인덱스 50: 1\n",
            "인덱스 51: 1\n",
            "인덱스 52: 1\n",
            "인덱스 53: 1\n",
            "인덱스 54: 1\n",
            "인덱스 55: 1\n",
            "인덱스 56: 1\n",
            "인덱스 57: 1\n",
            "인덱스 58: 1\n",
            "인덱스 59: 1\n",
            "인덱스 60: 1\n",
            "인덱스 61: 1\n",
            "인덱스 62: 1\n",
            "인덱스 63: 1\n",
            "인덱스 64: 1\n",
            "인덱스 65: 1\n",
            "인덱스 66: 1\n",
            "인덱스 67: 1\n",
            "인덱스 68: 1\n",
            "인덱스 69: 1\n",
            "인덱스 70: 1\n",
            "인덱스 71: 1\n",
            "인덱스 72: 1\n",
            "인덱스 73: 1\n",
            "인덱스 74: 1\n",
            "인덱스 75: 1\n",
            "인덱스 76: 1\n",
            "인덱스 77: 1\n",
            "인덱스 78: 1\n",
            "인덱스 79: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bieO3YwVvmLY"
      },
      "source": [
        "#### **Multi Head Attention 아키텍처**\n",
        "\n",
        "* 어텐션(attention)은 <b>세 가지 요소</b>를 입력으로 받습니다.\n",
        "    * <b>쿼리(queries)</b>\n",
        "    * <b>키(keys)</b>\n",
        "    * <b>값(values)</b>\n",
        "    * 현재 구현에서는 Query, Key, Value의 차원이 모두 같습니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRX0AoF1voKW"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        assert hidden_dim % n_heads == 0\n",
        "\n",
        "        self.hidden_dim = hidden_dim # 임베딩 차원\n",
        "        self.n_heads = n_heads # 헤드(head)의 개수: 서로 다른 어텐션(attention) 컨셉의 수\n",
        "        self.head_dim = hidden_dim // n_heads # 각 헤드(head)에서의 임베딩 차원\n",
        "\n",
        "        self.fc_q = nn.Linear(hidden_dim, hidden_dim) # Query 값에 적용될 FC 레이어\n",
        "        self.fc_k = nn.Linear(hidden_dim, hidden_dim) # Key 값에 적용될 FC 레이어\n",
        "        self.fc_v = nn.Linear(hidden_dim, hidden_dim) # Value 값에 적용될 FC 레이어\n",
        "\n",
        "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # query: [batch_size, query_len, hidden_dim]\n",
        "        # key: [batch_size, key_len, hidden_dim]\n",
        "        # value: [batch_size, value_len, hidden_dim]\n",
        " \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        # Q: [batch_size, query_len, hidden_dim]\n",
        "        # K: [batch_size, key_len, hidden_dim]\n",
        "        # V: [batch_size, value_len, hidden_dim]\n",
        "\n",
        "        # hidden_dim → n_heads X head_dim 형태로 변형\n",
        "        # n_heads(h)개의 서로 다른 어텐션(attention) 컨셉을 학습하도록 유도\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Q: [batch_size, n_heads, query_len, head_dim]\n",
        "        # K: [batch_size, n_heads, key_len, head_dim]\n",
        "        # V: [batch_size, n_heads, value_len, head_dim]\n",
        "\n",
        "        # Attention Energy 계산\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "        # energy: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # 마스크(mask)를 사용하는 경우\n",
        "        if mask is not None:\n",
        "            # 마스크(mask) 값이 0인 부분을 -1e10으로 채우기\n",
        "            energy = energy.masked_fill(mask==0, -1e10)\n",
        "\n",
        "        # 어텐션(attention) 스코어 계산: 각 단어에 대한 확률 값\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "\n",
        "        # attention: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # 여기에서 Scaled Dot-Product Attention을 계산\n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "\n",
        "        # x: [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        # x: [batch_size, query_len, n_heads, head_dim]\n",
        "\n",
        "        x = x.view(batch_size, -1, self.hidden_dim)\n",
        "\n",
        "        # x: [batch_size, query_len, hidden_dim]\n",
        "\n",
        "        x = self.fc_o(x)\n",
        "\n",
        "        # x: [batch_size, query_len, hidden_dim]\n",
        "\n",
        "        return x, attention"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzSDUlmOvq6-"
      },
      "source": [
        "#### **Position-wise Feedforward 아키텍처**\n",
        "\n",
        "* 입력과 출력의 차원이 동일합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBfNsiED6LSe"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "\n",
        "        # x: [batch_size, seq_len, pf_dim]\n",
        "\n",
        "        x = self.fc_2(x)\n",
        "\n",
        "        # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        return x"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz6fWXndvvwJ"
      },
      "source": [
        "#### **인코더(Encoder) 레이어 아키텍처**\n",
        "\n",
        "* 하나의 인코더 레이어에 대해 정의합니다.\n",
        "    * 입력과 출력의 차원이 같습니다.\n",
        "    * 이러한 특징을 이용해 트랜스포머의 인코더는 인코더 레이어를 여러 번 중첩해 사용합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율\n",
        "* &lt;pad&gt; 토큰에 대하여 마스크(mask) 값을 0으로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwhr1-df6MKk"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 하나의 임베딩이 복제되어 Query, Key, Value로 입력되는 방식\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        # self attention\n",
        "        # 필요한 경우 마스크(mask) 행렬을 이용하여 어텐션(attention)할 단어를 조절 가능\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "\n",
        "        # dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # position-wise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "\n",
        "        # dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        return src"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N22uqlEmvx06"
      },
      "source": [
        "#### **인코더(Encoder) 아키텍처**\n",
        "\n",
        "* 전체 인코더 아키텍처를 정의합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **input_dim**: 하나의 단어에 대한 원 핫 인코딩 차원\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_layers**: 내부적으로 사용할 인코더 레이어의 개수\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율\n",
        "    * **max_length**: 문장 내 최대 단어 개수\n",
        "* 원본 논문과는 다르게 <b>위치 임베딩(positional embedding)을 학습</b>하는 형태로 구현합니다.\n",
        "    * BERT와 같은 모던 트랜스포머 아키텍처에서 사용되는 방식입니다.\n",
        "* &lt;pad&gt; 토큰에 대하여 마스크(mask) 값을 0으로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j2ZRWaf6M14"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "        # pos: [batch_size, src_len]\n",
        "\n",
        "        # 소스 문장의 임베딩과 위치 임베딩을 더한 것을 사용\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # 모든 인코더 레이어를 차례대로 거치면서 순전파(forward) 수행\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        return src # 마지막 레이어의 출력을 반환"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl-eh0mAvzva"
      },
      "source": [
        "#### **디코더(Decoder) 레이어 아키텍처**\n",
        "\n",
        "* 하나의 디코더 레이어에 대해 정의합니다.\n",
        "    * 입력과 출력의 차원이 같습니다.\n",
        "    * 이러한 특징을 이용해 트랜스포머의 디코더는 디코더 레이어를 여러 번 중첩해 사용합니다.\n",
        "    * 디코더 레이어에서는 두 개의 Multi-Head Attention 레이어가 사용됩니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율\n",
        "* 소스 문장의 &lt;pad&gt; 토큰에 대하여 마스크(mask) 값을 0으로 설정합니다.\n",
        "* 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBalCQMq6Nj4"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\n",
        "        # trg_mask: [batch_size, trg_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        # self attention\n",
        "        # 자기 자신에 대하여 어텐션(attention)\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "\n",
        "        # dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "        # encoder attention\n",
        "        # 디코더의 쿼리(Query)를 이용해 인코더를 어텐션(attention)\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "\n",
        "        # dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "        # positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "\n",
        "        # dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        return trg, attention"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kRYUk-1v1jv"
      },
      "source": [
        "#### **디코더(Decoder) 아키텍처**\n",
        "\n",
        "* 전체 디코더 아키텍처를 정의합니다.\n",
        "* 하이퍼 파라미터(hyperparameter)\n",
        "    * **output_dim**: 하나의 단어에 대한 원 핫 인코딩 차원\n",
        "    * **hidden_dim**: 하나의 단어에 대한 임베딩 차원\n",
        "    * **n_layers**: 내부적으로 사용할 인코더 레이어의 개수\n",
        "    * **n_heads**: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
        "    * **pf_dim**: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "    * **dropout_ratio**: 드롭아웃(dropout) 비율\n",
        "    * **max_length**: 문장 내 최대 단어 개수\n",
        "* 원본 논문과는 다르게 <b>위치 임베딩(positional embedding)을 학습</b>하는 형태로 구현합니다.\n",
        "    * BERT와 같은 모던 트랜스포머 아키텍처에서 사용되는 방식입니다.\n",
        "* Seq2Seq과는 마찬가지로 실제로 추론(inference) 시기에서는 디코더를 반복적으로 넣을 필요가 있습니다.\n",
        "    * 학습(training) 시기에서는 한 번에 출력 문장을 구해 학습할 수 있습니다.\n",
        "* 소스 문장의 &lt;pad&gt; 토큰에 대하여 마스크(mask) 값을 0으로 설정합니다.\n",
        "* 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzJ-DCDt6Oev"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "\n",
        "        # trg: [batch_size, trg_len]\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\n",
        "        # trg_mask: [batch_size, trg_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "        # pos: [batch_size, trg_len]\n",
        "\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # 소스 마스크와 타겟 마스크 모두 사용\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        output = self.fc_out(trg)\n",
        "\n",
        "        # output: [batch_size, trg_len, output_dim]\n",
        "\n",
        "        return output, attention"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn-yCx4yv348"
      },
      "source": [
        "#### **트랜스포머(Transformer) 아키텍처**\n",
        "\n",
        "* 최종적인 전체 트랜스포머(Transformer) 모델을 정의합니다.\n",
        "* 입력이 들어왔을 때 앞서 정의한 인코더와 디코더를 거쳐 출력 문장을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ignEkCL6PSr"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    # 소스 문장의 <pad> 토큰에 대하여 마스크(mask) 값을 0으로 설정\n",
        "    def make_src_mask(self, src):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\n",
        "\n",
        "        return src_mask\n",
        "\n",
        "    # 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용\n",
        "    def make_trg_mask(self, trg):\n",
        "\n",
        "        # trg: [batch_size, trg_len]\n",
        "\n",
        "        \"\"\" (마스크 예시)\n",
        "        1 0 0 0 0\n",
        "        1 1 0 0 0\n",
        "        1 1 1 0 0\n",
        "        1 1 1 0 0\n",
        "        1 1 1 0 0\n",
        "        \"\"\"\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
        "\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        \"\"\" (마스크 예시)\n",
        "        1 0 0 0 0\n",
        "        1 1 0 0 0\n",
        "        1 1 1 0 0\n",
        "        1 1 1 1 0\n",
        "        1 1 1 1 1\n",
        "        \"\"\"\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "\n",
        "        # trg_sub_mask: [trg_len, trg_len]\n",
        "\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "        # trg: [batch_size, trg_len]\n",
        "\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # output: [batch_size, trg_len, output_dim]\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        return output, attention"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1TDOflzv5xW"
      },
      "source": [
        "#### **학습(Training)**\n",
        "\n",
        "* 하이퍼 파라미터 설정 및 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ombHNgO56Q7r"
      },
      "source": [
        "INPUT_DIM = len(korean_voca.word2idx)\n",
        "OUTPUT_DIM = len(english_voca.word2idx)\n",
        "HIDDEN_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W-Tr-D06SbJ"
      },
      "source": [
        "SRC_PAD_IDX = korean_voca.word2idx[korean_voca.PAD]\n",
        "TRG_PAD_IDX = english_voca.word2idx[english_voca.PAD]\n",
        "\n",
        "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
        "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
        "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
        "\n",
        "# Transformer 객체 선언\n",
        "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1DbVBkVwCvA"
      },
      "source": [
        "* **모델 가중치 파라미터 초기화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XObg1hxB6bOA",
        "outputId": "ef5dffc9-9486-4374-c06c-ced3503abddd"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 32,738,721 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWItEfu26chy",
        "outputId": "2ff71dd9-a965-4428-858a-7ce452d02827"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(40612, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(35745, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=35745, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Je4gMnSwITT"
      },
      "source": [
        "* 학습 및 평가 함수 정의\n",
        "    * 기본적인 Seq2Seq 모델과 거의 유사하게 작성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_hn64HD6dYK"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Adam optimizer로 학습 최적화\n",
        "LEARNING_RATE = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg6MC3dK6gF9"
      },
      "source": [
        "# 모델 학습(train) 함수\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train() # 학습 모드\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # 전체 학습 데이터를 확인하며\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch[0]\n",
        "        trg = batch[1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
        "        # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "\n",
        "        # output: [배치 크기, trg_len - 1, output_dim]\n",
        "        # trg: [배치 크기, trg_len]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "        # output: [배치 크기 * trg_len - 1, output_dim]\n",
        "        # trg: [배치 크기 * trg len - 1]\n",
        "\n",
        "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward() # 기울기(gradient) 계산\n",
        "\n",
        "        # 기울기(gradient) clipping 진행\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 전체 손실 값 계산\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEukz7xc6o6F"
      },
      "source": [
        "# 모델 평가(evaluate) 함수\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval() # 평가 모드\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 전체 평가 데이터를 확인하며\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch[0]\n",
        "            trg = batch[1]\n",
        "\n",
        "            # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
        "            # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "\n",
        "            # output: [배치 크기, trg_len - 1, output_dim]\n",
        "            # trg: [배치 크기, trg_len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            # output: [배치 크기 * trg_len - 1, output_dim]\n",
        "            # trg: [배치 크기 * trg len - 1]\n",
        "\n",
        "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # 전체 손실 값 계산\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4-D3IuNwO9t"
      },
      "source": [
        "* 학습(training) 및 검증(validation) 진행\n",
        "    * **학습 횟수(epoch)**: 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuAulfy_6pow"
      },
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC-dgh1E6qbF",
        "outputId": "101b897e-9b79-420e-ebcc-8e249cf9895f"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "N_EPOCHS = 4\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time() # 시작 시간 기록\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    end_time = time.time() # 종료 시간 기록\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'transformer_korean_to_english.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation PPL: {math.exp(valid_loss):.3f}')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 2m 49s\n",
            "\tTrain Loss: 6.369 | Train PPL: 583.557\n",
            "\tValidation Loss: 5.967 | Validation PPL: 390.197\n",
            "Epoch: 02 | Time: 2m 54s\n",
            "\tTrain Loss: 5.046 | Train PPL: 155.389\n",
            "\tValidation Loss: 5.631 | Validation PPL: 279.020\n",
            "Epoch: 03 | Time: 2m 56s\n",
            "\tTrain Loss: 4.339 | Train PPL: 76.621\n",
            "\tValidation Loss: 5.553 | Validation PPL: 257.931\n",
            "Epoch: 04 | Time: 2m 56s\n",
            "\tTrain Loss: 3.765 | Train PPL: 43.150\n",
            "\tValidation Loss: 5.578 | Validation PPL: 264.498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw1ntWEV6uth"
      },
      "source": [
        "# 번역(translation) 함수\n",
        "def translate_sentence(korean, model, device, max_len=80, logging=True):\n",
        "    model.eval() # 평가 모드\n",
        "\n",
        "    korean = clean_string(korean)\n",
        "    tokens = [line[0] for line in okt.pos(korean, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n",
        "    tokens = [korean_voca.SOS] + tokens + [korean_voca.EOS]\n",
        "    if logging:\n",
        "        print(f\"전체 소스 토큰: {tokens}\")\n",
        "\n",
        "    src_indexes = []\n",
        "    for token in tokens:\n",
        "        try:\n",
        "            src_indexes.append(korean_voca.word2idx[token])\n",
        "        except KeyError:\n",
        "            src_indexes.append(korean_voca.word2idx[korean_voca.UNK])\n",
        "    if logging:\n",
        "        print(f\"소스 문장 인덱스: {src_indexes}\")\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    # 소스 문장에 따른 마스크 생성\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    # 인코더(endocer)에 소스 문장을 넣어 문맥 벡터(context vector) 계산\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n",
        "    trg_indexes = [english_voca.word2idx[english_voca.SOS]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        # 출력 문장에 따른 마스크 생성\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # 출력 문장에서 가장 마지막 단어만 사용\n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
        "\n",
        "        # <eos>를 만나는 순간 끝\n",
        "        if pred_token == english_voca.word2idx[english_voca.EOS]:\n",
        "            break\n",
        "\n",
        "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
        "    trg_tokens = [english_voca.idx2word[i] for i in trg_indexes]\n",
        "\n",
        "    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avx76dMr9mXm",
        "outputId": "aa7229ce-49bd-4048-a4b5-03420464f422"
      },
      "source": [
        "example_idx = 15\n",
        "\n",
        "src = korean_lines_test[example_idx]\n",
        "trg = english_lines_test[example_idx]\n",
        "\n",
        "print(f'소스 문장: {src}', end='')\n",
        "print(f'타겟 문장: {trg}', end='')\n",
        "\n",
        "translation, attention = translate_sentence(src, model, device, logging=True)\n",
        "\n",
        "print(\"모델 출력 결과:\", \" \".join(translation))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소스 문장: 도쿄의 니케이 지수는 9291.03으로 0.33 퍼센트 하락했다.\n",
            "타겟 문장: Tokyo's Nikkei is off 0.33 percent to 9291.03.\n",
            "전체 소스 토큰: ['<sos>', '도쿄', '의', '니', '케이', '지수', '는', '929103', '으로', '033', '퍼센트', '하락', '했다', '<eos>']\n",
            "소스 문장 인덱스: [2, 29476, 1865, 6237, 9005, 8111, 18226, 0, 1052, 0, 29682, 6964, 31633, 3]\n",
            "모델 출력 결과: the dow jones industrial average fell up 65 percent <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2360KqGAcHNX"
      },
      "source": [
        "* 어텐션 맵(Attention Map) 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEhp1bcE33uU"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def display_attention(sentence, translation, attention, n_heads=8, n_rows=4, n_cols=2):\n",
        "\n",
        "    assert n_rows * n_cols == n_heads\n",
        "\n",
        "    plt.rc('font', family='NanumBarunGothic') # 폰트 설정\n",
        "    fig = plt.figure(figsize=(15, 25)) # 출력할 그림 크기 조절\n",
        "\n",
        "    for i in range(n_heads):\n",
        "        ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
        "\n",
        "        # 어텐션(Attention) 스코어 확률 값을 이용해 그리기\n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in sentence] + ['<eos>'], rotation=45)\n",
        "        ax.set_yticklabels([''] + translation)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzlH8RZEcIJF",
        "outputId": "9e9f97bf-706a-4d9a-b364-38bb90ac5607"
      },
      "source": [
        "src = \"남한과 미국은 동맹적인 관계를 유지하고 있다.\"\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "\n",
        "translation, attention = translate_sentence(src, model, device, logging=True)\n",
        "\n",
        "print(\"모델 출력 결과:\", \" \".join(translation))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소스 문장: 남한과 미국은 동맹적인 관계를 유지하고 있다.\n",
            "전체 소스 토큰: ['<sos>', '남한', '과', '미국', '은', '동맹', '적', '인', '관계', '를', '유지', '하고', '있다', '<eos>']\n",
            "소스 문장 인덱스: [2, 22119, 22931, 18435, 17445, 34393, 4176, 31515, 19807, 3084, 6209, 12265, 28480, 3]\n",
            "모델 출력 결과: the korea times has a political alliance and the united states <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dnwiCsNlcKkU",
        "outputId": "b3428dc9-7c47-467d-dcf3-3a204de91082"
      },
      "source": [
        "src = clean_string(src)\n",
        "korean_tokens = [line[0] for line in okt.pos(src, norm=True)] # 한글 형태소 분석 결과 추출\n",
        "\n",
        "display_attention(korean_tokens, translation, attention)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45224 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54620 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44284 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48120 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44397 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51008 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46041 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47609 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51064 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44288 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44228 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47484 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50976 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51648 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45224 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54620 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44284 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48120 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44397 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51008 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46041 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47609 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51064 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44288 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44228 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47484 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50976 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51648 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1800 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdEp459kf2Bz"
      },
      "source": [
        "#### <b>BLEU Score 계산</b>\n",
        "\n",
        "* 학습된 트랜스포머(Transformer) 모델의 BLEU 스코어 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76pGgi0IcS6M"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def show_bleu(model, device, max_len=80):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    index = 0\n",
        "\n",
        "    for i in range(len(korean_lines_test)):\n",
        "        src = korean_lines_test[i]\n",
        "        trg = english_lines_test[i]\n",
        "\n",
        "        trg = clean_string(trg)\n",
        "        trg = trg.split(' ')\n",
        "\n",
        "        pred_trg, _ = translate_sentence(src, model, device, max_len, logging=False)\n",
        "\n",
        "        # 마지막 <eos> 토큰 제거\n",
        "        pred_trg = pred_trg[:-1]\n",
        "\n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "\n",
        "        index += 1\n",
        "        if (index + 1) % 100 == 0:\n",
        "            print(f\"[{index + 1}/{len(korean_lines_test)}]\")\n",
        "            print(f\"예측: {pred_trg}\")\n",
        "            print(f\"정답: {trg}\")\n",
        "\n",
        "    bleu = bleu_score(pred_trgs, trgs, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
        "    print(f'Total BLEU Score = {bleu*100:.2f}')\n",
        "\n",
        "    individual_bleu1_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1, 0, 0, 0])\n",
        "    individual_bleu2_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 1, 0, 0])\n",
        "    individual_bleu3_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 0, 1, 0])\n",
        "    individual_bleu4_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[0, 0, 0, 1])\n",
        "\n",
        "    print(f'Individual BLEU1 score = {individual_bleu1_score*100:.2f}') \n",
        "    print(f'Individual BLEU2 score = {individual_bleu2_score*100:.2f}') \n",
        "    print(f'Individual BLEU3 score = {individual_bleu3_score*100:.2f}') \n",
        "    print(f'Individual BLEU4 score = {individual_bleu4_score*100:.2f}') \n",
        "\n",
        "    cumulative_bleu1_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1, 0, 0, 0])\n",
        "    cumulative_bleu2_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1/2, 1/2, 0, 0])\n",
        "    cumulative_bleu3_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1/3, 1/3, 1/3, 0])\n",
        "    cumulative_bleu4_score = bleu_score(pred_trgs, trgs, max_n=4, weights=[1/4, 1/4, 1/4, 1/4])\n",
        "\n",
        "    print(f'Cumulative BLEU1 score = {cumulative_bleu1_score*100:.2f}') \n",
        "    print(f'Cumulative BLEU2 score = {cumulative_bleu2_score*100:.2f}') \n",
        "    print(f'Cumulative BLEU3 score = {cumulative_bleu3_score*100:.2f}') \n",
        "    print(f'Cumulative BLEU4 score = {cumulative_bleu4_score*100:.2f}') "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nuh4aVgCf3MU",
        "outputId": "3a81785c-9ae0-4a88-a886-38c29f0084d8"
      },
      "source": [
        "show_bleu(model, device)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100/2000]\n",
            "예측: ['the', 'discovery', 'of', 'the', 'images', 'found', 'that', 'it', 'was', 'the', 'first', 'of', 'the', 'dna', 'of', 'the', 'mines', 'by', 'the', 'spanish', 'art', 'of', 'the', 'animals', 'in', 'the', 'wild', 'fauna', 'and', 'flora']\n",
            "정답: ['the', 'skull', 'nicknamed', 'toumai', 'found', 'in', 'northern', 'chad', 'by', 'an', 'international', 'team', 'is', 'not', 'part', 'of', 'the', 'human', 'family', 'tree']\n",
            "[200/2000]\n",
            "예측: ['the', 'mother', 'of', 'her', 'husband', 'and', 'his', 'family', 'had', 'been', 'killed', 'in', 'the', 'late', 'edition', 'of', 'the', 'bahamas', 'on', 'sunday', 'morning']\n",
            "정답: ['johnsons', 'eviscerated', 'body', 'which', 'police', 'said', 'they', 'found', 'after', 'receiving', 'calls', 'about', 'a', 'foul', 'odor', 'coming', 'from', 'the', 'apartment', 'was', 'in', 'a', 'state', 'of', 'moderate', 'decomposition', 'and', 'she', 'had', 'been', 'dead', 'about', 'two', 'days', 'medical', 'examiner', 'karl', 'williams', 'said']\n",
            "[300/2000]\n",
            "예측: ['the', 'study', 'found', 'that', 'more', 'than', '50', 'percent', 'of', 'the', 'health', 'ministry', 'said', 'the', 'study', 'was', 'likely', 'to', 'be', 'released']\n",
            "정답: ['studies', 'show', 'productivity', 'drops', 'when', 'temperatures', 'dip', 'below', '72', 'degrees']\n",
            "[400/2000]\n",
            "예측: ['he', 'said', 'he', 'had', 'arrested', 'three', 'children', 'on', 'charges', 'of', 'illegally', 'raising', 'a', 'slush', 'fund', 'scandal', 'and', 'charged', 'with', 'misdemeanor', 'counts', 'of', 'corruption', 'charges']\n",
            "정답: ['the', 'couple', 'who', 'face', 'charges', 'of', 'bodily', 'harm', 'making', 'threats', 'and', 'coercion', 'were', 'released', 'on', 'bail', 'thursday', 'he', 'said']\n",
            "[500/2000]\n",
            "예측: ['the', 'israeli', 'army', 'has', 'been', 'killed', 'in', 'the', 'violence', 'since', 'the', 'israeli', 'army', 'was', 'arrested']\n",
            "정답: ['israel', 'on', 'wednesday', 'also', 'released', 'the', 'remains', 'of', '199', 'fighters', 'from', 'lebanon']\n",
            "[600/2000]\n",
            "예측: ['the', 'us', 'geological', 'survey', 'said', 'the', 'quake', 'was', 'destroyed', 'by', 'the', 'us', 'geological', 'survey']\n",
            "정답: ['a', 'strong', 'earthquake', 'struck', 'monday', 'near', 'some', 'greek', 'islands', 'close', 'to', 'the', 'turkish', 'coast', 'according', 'to', 'the', 'us', 'geological', 'service']\n",
            "[700/2000]\n",
            "예측: ['the', 'announcement', 'came', 'after', 'the', 'last', 'month', 'that', 'the', 'company', 'had', 'been', 'paid', 'for', 'the', 'last', 'month', 'but', 'failed', 'to', 'sell', 'it', 'to', 'a', 'price']\n",
            "정답: ['inbevs', 'original', 'offer', 'of', '46', 'billion', 'made', 'on', 'june', '11th', 'was', 'rejected', 'as', 'too', 'low']\n",
            "[800/2000]\n",
            "예측: ['but', 'they', 'were', 'able', 'to', 'get', 'a', 'cat', 'and', 'to', 'get', 'a', 'cat', 'out', 'of', 'the', 'red', 'cross']\n",
            "정답: ['but', 'she', 'points', 'out', 'that', 'the', 'jewelry', 'is', 'almost', 'always', 'made', 'from', 'cheap', 'metal', 'that', 'will', 'turn', 'yellow', 'or', 'lose', 'its', 'sheen', 'within', 'weeks']\n",
            "[900/2000]\n",
            "예측: ['iran', 'has', 'been', 'reported', 'in', 'a', 'missile', 'tests', 'on', 'a', 'missile', 'defense', 'official', 'said']\n",
            "정답: ['iranian', 'media', 'reported', 'that', 'tehran', 'fired', 'a', 'series', 'of', 'missiles', 'on', 'thursday', 'in', 'a', 'second', 'day', 'of', 'longrange', 'missile', 'testing']\n",
            "[1000/2000]\n",
            "예측: ['police', 'said', 'they', 'were', 'trying', 'to', 'kill', 'the', 'woman', 'and', 'that', 'women', 'were', 'killed', 'in', 'a', 'suicide', 'bomber', 'who', 'died', 'after', 'a', 'suicide', 'bomber', 'killed', 'her', 'mother', 'and', 'killed', 'her', 'mother']\n",
            "정답: ['when', 'police', 'surrounded', 'the', 'men¡¯s', 'apartment', 'tuesday', 'they', 'found', '15', 'men', 'and', 'women', 'wielding', 'knives', 'and', 'shouting', '¡°sacrifice', 'for', 'allah¡±', 'a', 'police', 'spokesman', 'told', 'xinhua']\n",
            "[1100/2000]\n",
            "예측: ['north', 'korea', 'has', 'been', 'criticized', 'for', 'failing', 'to', 'dismantle', 'its', 'nuclear', 'program']\n",
            "정답: ['north', 'korea', 'declared', 'details', 'of', 'its', 'nuclear', 'program', 'last', 'month']\n",
            "[1200/2000]\n",
            "예측: ['president', 'george', 'w', 'bush', 'has', 'said', 'he', 'was', 'a', 'very', 'important', 'step', 'to', 'the', 'us', 'president']\n",
            "정답: ['president', 'bush', 'gave', 'a', 'positive', 'but', 'cautious', 'assessment', 'of', 'russia¡¯s', 'new', 'president', 'dmitry', 'medvedev']\n",
            "[1300/2000]\n",
            "예측: ['he', 'also', 'said', 'the', 'world', 'number', 'of', 'scientists', 'are', 'being', 'used', 'by', 'the', 'worlds', 'largest', 'number', 'of', 'children', 'and', 'the', 'worlds', 'most', 'dangerous', 'levels']\n",
            "정답: ['von', 'hagens', 'says', 'he', 'relies', 'on', 'donors', 'not', 'only', 'as', 'a', 'source', 'of', 'specimens', 'but', 'also', 'as', 'representations', 'of', 'body', 'worlds', 'philosophy']\n",
            "[1400/2000]\n",
            "예측: ['ma', 'won', 'the', 'opening', 'ceremony', 'of', 'the', 'chinese', 'premier', 'wen', 'jiabao', 'and', 'the', 'communist', 'party']\n",
            "정답: ['taiwans', 'new', 'president', 'ma', 'yingjeou', 'has', 'rejected', 'the', 'push', 'for', 'independence']\n",
            "[1500/2000]\n",
            "예측: ['the', 'new', 'york', 'times', 'says', 'the', 'new', 'york', 'times', 'have', 'a', 'chance', 'of', 'having', 'sex', 'in', 'the', 'world', 'health', 'organization', 'that', 'has', 'been', 'on', 'the', 'internet']\n",
            "정답: ['walter', 'scott', '24', 'put', 'his', 'soul', 'up', 'for', 'sale', 'on', 'new', 'zealand', 'internet', 'auction', 'site', 'trademe', 'and', 'so', 'far', 'has', 'received', 'more', 'than', '100', 'expressions', 'of', 'interest']\n",
            "[1600/2000]\n",
            "예측: ['the', 'united', 'states', 'has', 'urged', 'north', 'korea', 'to', 'take', 'a', 'nuclear', 'standoff', 'over', 'its', 'nuclear', 'weapons', 'program']\n",
            "정답: ['but', 'the', 'governing', 'uri', 'party', 'still', 'underlined', 'the', 'importance', 'of', 'interkorean', 'economic', 'projects']\n",
            "[1700/2000]\n",
            "예측: ['authorities', 'said', 'they', 'were', 'killed', 'in', 'a', 'remote', 'town', 'of', 'talafar', 'near', 'the', 'border', 'with', 'the', 'military']\n",
            "정답: ['regional', 'officials', 'said', 'he', 'died', 'in', 'a', 'shootout', 'when', 'marines', 'swooped', 'on', 'his', 'hideout', 'in', 'the', 'island', 'of', 'tawitawi']\n",
            "[1800/2000]\n",
            "예측: ['the', 'united', 'states', 'has', 'said', 'it', 'would', 'not', 'be', 'allowed', 'to', 'take', 'a', 'timetable', 'for', 'the', 'un']\n",
            "정답: ['the', 'move', 'was', 'opposed', 'by', 'the', 'party', 'of', 'president', 'nicolas', 'sarkozy', 'who', 'has', 'been', 'trying', 'to', 'ease', 'tense', 'ties', 'with', 'beijing']\n",
            "[1900/2000]\n",
            "예측: ['the', 'dow', 'is', 'up', 'about', '25', 'points']\n",
            "정답: ['right', 'now', 'the', 'dow', 'is', 'up', '14', 'points']\n",
            "[2000/2000]\n",
            "예측: ['a', 'train', 'explosion', 'on', 'a', 'train', 'station', 'in', 'tokyo']\n",
            "정답: ['japans', 'derailed', 'commuter', 'train', 'accident', 'has', 'killed', 'at', 'least', '69', 'people']\n",
            "Total BLEU Score = 1.93\n",
            "Individual BLEU1 score = 21.10\n",
            "Individual BLEU2 score = 3.59\n",
            "Individual BLEU3 score = 0.83\n",
            "Individual BLEU4 score = 0.22\n",
            "Cumulative BLEU1 score = 21.10\n",
            "Cumulative BLEU2 score = 8.70\n",
            "Cumulative BLEU3 score = 3.97\n",
            "Cumulative BLEU4 score = 1.93\n"
          ]
        }
      ]
    }
  ]
}